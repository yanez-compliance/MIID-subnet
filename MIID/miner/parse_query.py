"""
Utilities to parse query templates produced by `MIID.validator.query_generator.QueryGenerator`.

Features:
- LLM-based parsing via OpenRouter API (default)
- Regex-based fallback when LLM is unavailable

Extracts:
- variation_count
- phonetic_similarity distribution
- orthographic_similarity distribution
- rule_percentage
- selected_rules (mapped back to internal rule names when recognizable)

Designed to be tolerant to wording differences while following the
formats generated by `QueryGenerator` (default, simple fallback, or LLM-generated).
"""

from __future__ import annotations

import asyncio
import json
from math import e
import os
import random
import re
from typing import Any, Dict, List, Optional
import logging

from openai import AsyncOpenAI
from MIID.validator.rule_extractor import RULE_DESCRIPTIONS
import bittensor as bt

logger = logging.getLogger(__name__)

# Reverse map: description -> rule_name for easy lookup
DESCRIPTION_TO_RULE: Dict[str, str] = {
    description.lower(): rule_name for rule_name, description in RULE_DESCRIPTIONS.items()
}

# Available models for OpenRouter API
AVAILABLE_MODELS = [
    # "google/gemini-2.5-flash-lite-preview-06-17"
    # "google/gemini-2.5-pro"
    # "anthropic/claude-3.5-sonnet"
    # "anthropic/claude-sonnet-4"
    # "anthropic/claude-3.7-sonnet"
    "openai/gpt-4.1"
]

# -----------------------------
# OPENROUTER API UTILITIES
# -----------------------------

def _get_api_keys() -> List[str]:
    """Get API keys from environment variables"""
    api_keys = []
    
    # Try to get API keys from environment variables
    env_key = os.getenv('OPENROUTER_API_KEY')
    if env_key and env_key not in api_keys:
        api_keys.append(env_key)
    
    # Try to get multiple API keys from environment
    env_keys = os.getenv('OPENROUTER_API_KEYS')
    if env_keys:
        env_key_list = [key.strip() for key in env_keys.split(',') if key.strip()]
        for key in env_key_list:
            if key not in api_keys:
                api_keys.append(key)
    
    # If no API keys found, use default
    if not api_keys:
        # default_key = "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"
        # default_key = "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"
        default_key = "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"
        
        api_keys.append(default_key)
    
    # Remove duplicates while preserving order
    unique_keys = []
    for key in api_keys:
        if key not in unique_keys:
            unique_keys.append(key)
    
    return unique_keys

def _initialize_clients() -> List[AsyncOpenAI]:
    """Initialize OpenRouter clients for each API key"""
    clients = []
    api_keys = _get_api_keys()
    
    for api_key in api_keys:
        try:
            client = AsyncOpenAI(
                base_url="https://openrouter.ai/api/v1",
                api_key=api_key,
            )
            clients.append(client)
        except Exception:
            # Continue with other clients even if one fails
            continue
    return clients

# -----------------------------
# LLM-BASED PARSING (OpenRouter)
# -----------------------------


def _build_llm_prompt(query_text: str) -> str:
    """Construct a concise instruction for the LLM to output strict JSON, with few-shot examples from recent errors."""
    id_list_only = "\n".join(f"- {rule_id}" for rule_id in RULE_DESCRIPTIONS.keys())
    rules_descriptions_json = json.dumps(RULE_DESCRIPTIONS, indent=4)


    return f"""
You are extracting parameters from a natural-language query about generating name variations.

Return ONLY a JSON object with fields:
{{
  "variation_count": <int not 0, valid range: 1-15>,
  "phonetic_similarity": {{"Light": <0.0-1.0>, "Medium": <0.0-1.0>, "Far": <0.0-1.0>}},
  "orthographic_similarity": {{"Light": <0.0-1.0>, "Medium": <0.0-1.0>, "Far": <0.0-1.0>}},
  "rule_percentage": <0.0 if not mentioned in query, otherwise 0.1-0.6>
}}

Rules:
- Use ONLY explicitly stated values. If values are given as percents, convert to decimals 0.0-1.0.
- If orthographic similarity is mentioned without per-level numbers, set {{"Light": 1.0}} unless the text clearly says significant/major/heavy (then {{"Far": 1.0}}).
- List at most 3 rule identifiers, only if explicitly requested.
- If rule percentage is not mentioned in query, set rule_percentage to 0.0 (don't guess/infer a value)
- For variation_count, extract explicit numbers like "generate 5 variations" or "create 10 names", "10 excutor vectors".

Input query:
{query_text}

Output: JSON only, no extra text.
"""



def _extract_json_from_text(text: str) -> Optional[Dict[str, Any]]:
    """Find the last JSON object in the text and parse it."""
    try:
        candidates = re.findall(r"\{[\s\S]*\}", text)
        if not candidates:
            return None
        for candidate in reversed(candidates):
            try:
                return json.loads(candidate)
            except Exception:
                continue
    except Exception:
        return None
    return None


def _extract_rules_from_query(query_text: str, rule_descriptions: Dict[str, str]) -> List[str]:
    """Extract rule IDs from query text by matching rule descriptions with flexible matching."""
    import re
    from itertools import permutations
    
    # Stop words to remove
    stop_words = {
        'a', 'an', 'last', 'first', 'the', 'in', 'on', 'at', 'to', 'for', 'with', 'name', 'target', "one", "or", "more", "is" ,
        "identity", "'s", "within"
    }
    
    def normalize_text(text: str) -> str:
        """Normalize text by removing braces, converting to lowercase, and cleaning whitespace."""
        # Remove braces and normalize
        # Replace words ending with -ed with just 'e'
        text = re.sub(r'\b(\w+)ed\b', r'\1e', text)
        text = re.sub(r'\b(\w+)s\b', r'\1', text)
        text = re.sub(r'\bone or more\b', 'random', text)
        text = re.sub(r'\btwo adjacent\b', 'random adjacent', text)
        text = re.sub(r'[{}]', '', text)
        text = text.lower().strip()
        return text
    
    def extract_meaningful_words(text: str) -> List[str]:
        """Extract meaningful words by removing stop words and punctuation."""
        # Split on whitespace and punctuation
        words = re.findall(r'\b\w+\b|,', text.lower())
        # Remove stop words
        meaningful_words = [word for word in words if word not in stop_words and (len(word) > 1 or word == ",")]
        return meaningful_words
    
    def generate_all_variants(words: List[str]) -> List[str]:
        """Generate all permutations of meaningful words as contiguous phrases."""
        if not words:
            return []
        # Limit permutations to avoid explosion
        if len(words) > 6:
            return []
        return [' '.join(p) for p in permutations(words)]
    
    # Normalize query text and build a contiguous string of meaningful words
    normalized_query = normalize_text(query_text)
    query_meaningful_tokens = extract_meaningful_words(normalized_query)
    query_meaningful_str = ' '.join(query_meaningful_tokens)
    print(query_meaningful_str)
    
    
    matched_rules: List[str] = []
    
    # Process each rule description
    for rule_id, description in rule_descriptions.items():
        normalized_desc = normalize_text(description)
        meaningful_words = extract_meaningful_words(normalized_desc)
        if not meaningful_words:
            continue
        
        # Generate all permutations and require a contiguous phrase match in the meaningful stream
        for variant in generate_all_variants(meaningful_words):
            if variant and variant in query_meaningful_str:
                matched_rules.append(rule_id)
                break
    
    # Return at most 3 rules
    return matched_rules[:3]


def _normalize_llm_result(obj: Dict[str, Any]) -> Dict[str, Any]:
    """Normalize the LLM JSON into the expected dictionary structure."""
    result: Dict[str, Any] = {
        "variation_count": None,
        "phonetic_similarity": {},
        "orthographic_similarity": {},
        "rule_percentage": None,
        "selected_rules": [],
    }

    # Variation count``
    vc = obj.get("variation_count")
    if not vc:
        vc = 5
    elif vc > 15:
        vc = 10
    result["variation_count"] = int(vc)

    def normalize_distribution(dist: Any) -> Dict[str, float]:
        if not isinstance(dist, dict):
            return {}
        values = {k.capitalize(): float(v) for k, v in dist.items() if k and isinstance(v, (int, float))}
        # Keep only recognized levels
        values = {k: v for k, v in values.items() if k in {"Light", "Medium", "Far"}}
        # If any value > 1, assume percentages and normalize by total to 0..1
        if any(v > 1.0 for v in values.values()):
            total = sum(values.values())
            if total > 0:
                values = {k: round(v / total, 6) for k, v in values.items()}
        return values

    result["phonetic_similarity"] = normalize_distribution(obj.get("phonetic_similarity", {}))
    result["orthographic_similarity"] = normalize_distribution(obj.get("orthographic_similarity", {}))

    # Rules: expect identifiers only; drop anything not in allowed list
    selected_rules: List[str] = []
    if isinstance(obj.get("rule_based_rules"), list):
        for item in obj["rule_based_rules"]:
            if isinstance(item, str) and item in RULE_DESCRIPTIONS:
                selected_rules.append(item)
    result["selected_rules"] = selected_rules

    rp = obj.get("rule_percentage")
    if isinstance(rp, (int, float)):
        # If >1, assume percentage and convert to decimal
        if rp == 0.0:
            rp =  (len(selected_rules) + 0.1) / result["variation_count"]
        
        result["rule_percentage"] = min(0.6, float(rp) / 100.0 if rp > 1 else float(rp))
    # warn with 0.09, 0.64, ...
    result["orthographic_similarity"]["Light"] = round(result.get("orthographic_similarity", {"Light": 0.0}).get("Light", 0.0), 1)
    result["orthographic_similarity"]["Medium"] = round(result.get("orthographic_similarity", {"Medium": 0.0}).get("Medium", 0.0), 1)
    result["orthographic_similarity"]["Far"] = round(result.get("orthographic_similarity", {"Far": 0.0}).get("Far", 0.0), 1)
    result["phonetic_similarity"]["Light"] = round(result.get("phonetic_similarity", {"Light": 0.0}).get("Light", 0.0), 1)
    result["phonetic_similarity"]["Medium"] = round(result.get("phonetic_similarity", {"Medium": 0.0}).get("Medium", 0.0), 1)
    result["phonetic_similarity"]["Far"] = round(result.get("phonetic_similarity", {"Far": 0.0}).get("Far", 0.0), 1)

    if result["phonetic_similarity"]["Light"] + result["phonetic_similarity"]["Medium"] + result["phonetic_similarity"]["Far"] < 1.0:
        if result["phonetic_similarity"]["Light"] == 0.2:
            result["phonetic_similarity"]["Medium"] = 0.6
            result["phonetic_similarity"]["Far"] = 0.2

        elif result["phonetic_similarity"]["Light"] == 0.1:
            if result["phonetic_similarity"]["Medium"] == 0.5:
                result["phonetic_similarity"]["Far"] = 0.4
            elif result["phonetic_similarity"]["Medium"] == 0.6:
                result["phonetic_similarity"]["Far"] = 0.3
            else:
                result["phonetic_similarity"]["Far"] = 0.4
                result["phonetic_similarity"]["Medium"] = 0.5

        elif result["phonetic_similarity"]["Light"] == 0.3:
            result["phonetic_similarity"]["Medium"] = 0.4
            result["phonetic_similarity"]["Far"] = 0.3

        elif result["phonetic_similarity"]["Light"] == 0.5:
            result["phonetic_similarity"]["Medium"] = 0.5
            result["phonetic_similarity"]["Far"] = 0.0

        elif result["phonetic_similarity"]["Light"] == 0.7:
            result["phonetic_similarity"]["Medium"] = 0.3
            result["phonetic_similarity"]["Far"] = 0.0

        elif result["phonetic_similarity"]["Medium"] == 0.4:
            result["phonetic_similarity"]["Light"] = 0.3
            result["phonetic_similarity"]["Far"] = 0.3

        elif result["phonetic_similarity"]["Far"] == 0.6:
            result["phonetic_similarity"]["Medium"] = 0.3
            result["phonetic_similarity"]["Far"] = 0.1

        elif result["phonetic_similarity"]["Far"] == 0.4:
            result["phonetic_similarity"]["Medium"] = 0.5
            result["phonetic_similarity"]["Far"] = 0.1

        elif result["phonetic_similarity"]["Light"] + result["phonetic_similarity"]["Medium"] + result["phonetic_similarity"]["Far"]  == 0.0:
            result["phonetic_similarity"]["Light"] = 0.3
            result["phonetic_similarity"]["Medium"] = 0.4
            result["phonetic_similarity"]["Far"] = 0.3

        elif result["phonetic_similarity"]["Light"] + result["phonetic_similarity"]["Medium"] + result["phonetic_similarity"]["Far"]  < 1.0:
            result["phonetic_similarity"]["Light"] = 0.3
            result["phonetic_similarity"]["Medium"] = 0.4
            result["phonetic_similarity"]["Far"] = 0.3

    if result["orthographic_similarity"]["Light"] + result["orthographic_similarity"]["Medium"] + result["orthographic_similarity"]["Far"] < 1.0:
        if result["orthographic_similarity"]["Light"] == 0.2:
            result["orthographic_similarity"]["Medium"] = 0.6
            result["orthographic_similarity"]["Far"] = 0.2

        elif result["orthographic_similarity"]["Light"] == 0.1:
            if result["orthographic_similarity"]["Medium"] == 0.5:
                result["orthographic_similarity"]["Far"] = 0.4
            elif result["orthographic_similarity"]["Medium"] == 0.6:
                result["orthographic_similarity"]["Far"] = 0.3
            else:
                result["orthographic_similarity"]["Far"] = 0.4
                result["orthographic_similarity"]["Medium"] = 0.5

        elif result["orthographic_similarity"]["Light"] == 0.3:
            result["orthographic_similarity"]["Medium"] = 0.4
            result["orthographic_similarity"]["Far"] = 0.3

        elif result["orthographic_similarity"]["Light"] == 0.5:
            result["orthographic_similarity"]["Medium"] = 0.5
            result["orthographic_similarity"]["Far"] = 0.0

        elif result["orthographic_similarity"]["Light"] == 0.7:
            result["orthographic_similarity"]["Medium"] = 0.3
            result["orthographic_similarity"]["Far"] = 0.0

        elif result["orthographic_similarity"]["Medium"] == 0.4:
            result["orthographic_similarity"]["Light"] = 0.3
            result["orthographic_similarity"]["Far"] = 0.3

        elif result["orthographic_similarity"]["Far"] == 0.6:
            result["orthographic_similarity"]["Medium"] = 0.3
            result["orthographic_similarity"]["Far"] = 0.1

        elif result["orthographic_similarity"]["Far"] == 0.4:
            result["orthographic_similarity"]["Medium"] = 0.5
            result["orthographic_similarity"]["Far"] = 0.1

        elif result["orthographic_similarity"]["Light"] + result["orthographic_similarity"]["Medium"] + result["orthographic_similarity"]["Far"]  == 0.0:
            result["orthographic_similarity"]["Light"] = 0.3
            result["orthographic_similarity"]["Medium"] = 0.4
            result["orthographic_similarity"]["Far"] = 0.3

        elif result["orthographic_similarity"]["Light"] + result["orthographic_similarity"]["Medium"] + result["orthographic_similarity"]["Far"]  < 1.0:
            result["orthographic_similarity"]["Light"] = 0.3
            result["orthographic_similarity"]["Medium"] = 0.4
            result["orthographic_similarity"]["Far"] = 0.3


        
    return result


async def parse_query_with_llm(
    query_text: str,
    *,
    clients: Optional[List[AsyncOpenAI]] = None,
    max_retries: int = 1,
) -> Dict[str, Any]:
    """Parse using OpenRouter LLM into a structured dict. Falls back to empty result on failure."""
    if not clients:
        clients = _initialize_clients()
    
    if not clients:
        # No clients available, return empty result
        return {
            "variation_count": 10,
            "phonetic_similarity": {"Light": 0.3, "Medium": 0.4, "Far": 0.3},
            "orthographic_similarity": {"Light": 0.3, "Medium": 0.4, "Far": 0.3},
            "rule_percentage": 0.3,
            "selected_rules": _extract_rules_from_query(query_text, RULE_DESCRIPTIONS),
        }
    
    prompt = _build_llm_prompt(query_text)
    
    for _ in range(max_retries + 1):
        try:
            # Randomly select a client and model
            client = random.choice(clients)
            selected_model = random.choice(AVAILABLE_MODELS)
            
            # Get response from LLM
            response = await client.chat.completions.create(
                model=selected_model,
                messages=[{
                    'role': 'user',
                    'content': prompt,
                }],
                temperature=0,
                top_p=1,
                stream=False,
                presence_penalty=0,
                frequency_penalty=1.0,
                max_tokens=1024
            )
            
            # Extract and parse response
            content = response.choices[0].message.content
            if not content:
                raise ValueError("Empty response from LLM")
            
            obj = _extract_json_from_text(content)
            
            if obj:
                llm_parsed = _normalize_llm_result(obj)
                # Populate selected_rules by matching description variants in the query text
                llm_parsed["selected_rules"] = _extract_rules_from_query(query_text, RULE_DESCRIPTIONS)
                return llm_parsed
            
        except Exception as e:
            bt.logging.error(f"Error parsing query with LLM: {e}")
            bt.logging.error(f"Response: {response}")
            continue
    
    # If here, LLM failed - return empty result
    return {
        "variation_count": 10,
        "phonetic_similarity": {"Light": 0.3, "Medium": 0.4, "Far": 0.3},
        "orthographic_similarity": {"Light": 0.3, "Medium": 0.4, "Far": 0.3},
        "rule_percentage": 0.3,
        "selected_rules": _extract_rules_from_query(query_text, RULE_DESCRIPTIONS),
    }


# -----------------------------
# REGEX-BASED PARSING (Fallback)
# -----------------------------

## Regex-based parsing removed per instruction: LLM-only parsing now

async def query_parser(
    query_text: str,
    *,
    clients: Optional[List[AsyncOpenAI]] = None,
    max_retries: int = 1,
) -> Dict[str, Any]:
    """Public API: LLM-only parsing interface."""
    result = await parse_query_with_llm(
        query_text,
        clients=clients,
        max_retries=max_retries,
    )
    return result

def query_parser_sync(
    query_text: str,
    *,
    clients: Optional[List[AsyncOpenAI]] = None,
    max_retries: int = 1,
) -> Dict[str, Any]:
    """Synchronous wrapper for parse_query for backwards compatibility."""
    return asyncio.run(query_parser(
        query_text,
        clients=clients,
        max_retries=max_retries,
    ))


__all__ = [
    "query_parser",
    "query_parser_sync", 
]
