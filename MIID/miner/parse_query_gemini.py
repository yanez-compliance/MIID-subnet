"""
Utilities to parse query templates produced by `MIID.validator.query_generator.QueryGenerator`.

Features:
- LLM-based parsing via Google Gemini API
- Regex-based fallback when LLM is unavailable

Extracts:
- variation_count
- phonetic_similarity distribution
- orthographic_similarity distribution
- rule_percentage
- selected_rules (mapped back to internal rule names when recognizable)

Designed to be tolerant to wording differences while following the
formats generated by `QueryGenerator` (default, simple fallback, or LLM-generated).
"""

from __future__ import annotations

import asyncio
import json
import os
import random
import re
from typing import Any, Dict, List, Optional
import logging

import google.generativeai as genai
from MIID.validator.rule_extractor import RULE_DESCRIPTIONS
import bittensor as bt

logger = logging.getLogger(__name__)

# Reverse map: description -> rule_name for easy lookup
DESCRIPTION_TO_RULE: Dict[str, str] = {
    description.lower(): rule_name for rule_name, description in RULE_DESCRIPTIONS.items()
}

# Available Gemini models
AVAILABLE_MODELS = [
    "gemini-1.5-flash",
    "gemini-1.5-pro",
    "gemini-2.0-flash-exp",
    "gemini-2.0-flash"
]

# -----------------------------
# GEMINI API UTILITIES
# -----------------------------

def _get_api_keys() -> List[str]:
    """Get API keys from environment variables"""
    api_keys = []
    
    # Try to get API keys from environment variables
    env_key = os.getenv('GOOGLE_API_KEY')
    if env_key and env_key not in api_keys:
        api_keys.append(env_key)
    
    # Try to get multiple API keys from environment
    env_keys = os.getenv('GOOGLE_API_KEYS')
    if env_keys:
        env_key_list = [key.strip() for key in env_keys.split(',') if key.strip()]
        for key in env_key_list:
            if key not in api_keys:
                api_keys.append(key)
    
    # If no API keys found, use default (you should set your own key)
    if not api_keys:
        # Replace with your actual Google API key
        default_key = "AIzaSyCvo_Pjngu84CFtyeBaU4J3eR_tpbuaLOI"
        api_keys.append(default_key)
    
    # Remove duplicates while preserving order
    unique_keys = []
    for key in api_keys:
        if key not in unique_keys:
            unique_keys.append(key)
    
    return unique_keys

def _initialize_clients() -> List[genai.GenerativeModel]:
    """Initialize Gemini clients for each API key"""
    clients = []
    api_keys = _get_api_keys()
    
    for api_key in api_keys:
        try:
            # Configure the API key
            genai.configure(api_key=api_key)
            
            # Create a model instance (we'll use the first available model)
            model = genai.GenerativeModel(AVAILABLE_MODELS[0])
            clients.append(model)
        except Exception as e:
            logger.warning(f"Failed to initialize Gemini client with key: {e}")
            # Continue with other clients even if one fails
            continue
    return clients

# -----------------------------
# LLM-BASED PARSING (Gemini)
# -----------------------------

def _extract_recent_mistake_examples(max_examples: int = 100) -> List[Dict[str, str]]:
    """Extract up to N recent (Template/Query, Expected/Answer) pairs from known logs.

    Supports files containing blocks with 'Template:' and 'Expected:' or with
    'Query:' and 'Answer:'. Returns newest examples first.
    """
    candidate_paths = [
        "/root/MIID-subnet/errored_queries_list.log",
    ]

    def parse_pairs(lines: List[str]) -> List[Dict[str, str]]:
        pairs: List[Dict[str, str]] = []
        capturing_template = False
        template_lines: List[str] = []
        for line in lines:
            stripped = line.strip()
            # Prefer explicit Template: if present
            if stripped.startswith("Template:"):
                capturing_template = True
                template_lines = [line.split("Template:", 1)[1]]
                continue
            # Fallback: Query:
            if not capturing_template and stripped.startswith("Query:"):
                capturing_template = True
                template_lines = [line.split("Query:", 1)[1]]
                continue
            if capturing_template:
                if stripped.startswith("Expected:") or stripped.startswith("Answer:"):
                    expected_str = (
                        line.split(":", 1)[1].strip()
                        if ":" in line
                        else stripped
                    )
                    raw_template = "".join(template_lines).strip()
                    raw_template = re.sub(r"\s+", " ", raw_template)
                    if len(raw_template) > 900:
                        raw_template = raw_template[:900] + " â€¦"
                    pairs.append({"template": raw_template, "expected_json": expected_str})
                    capturing_template = False
                    template_lines = []
                else:
                    template_lines.append(line)
        return pairs

    collected: List[Dict[str, str]] = []
    try:
        for path in candidate_paths:
            if not os.path.exists(path):
                continue
            with open(path, "r", encoding="utf-8", errors="ignore") as f:
                lines = f.readlines()
            collected.extend(parse_pairs(lines))
    except Exception:
        pass

    # Keep most recent
    if collected:
        collected = collected[-max_examples:]
        collected.reverse()
    return collected

def _build_llm_prompt(query_text: str) -> str:
    """Construct a concise instruction for the LLM to output strict JSON, with few-shot examples from recent errors."""
    id_list_only = "\n".join(f"- {rule_id}" for rule_id in RULE_DESCRIPTIONS.keys())

    examples = _extract_recent_mistake_examples()
    examples_text = ""
    if examples:
        blocks: List[str] = []
        for idx, ex in enumerate(examples, start=1):
            blocks.append(
                "Example {}:\nINPUT:\n{}\nOUTPUT:\n{}\n".format(
                    idx, ex["template"], ex["expected_json"]
                )
            )
        examples_text = (
            "Past failures and correct outputs (follow these patterns):\n"
            + "\n".join(blocks)
            + "\n"
        )

    return f"""
You are extracting parameters from a natural-language query about generating name variations.

Return ONLY a JSON object with fields:
{{
  "variation_count": <int not 0, valid range: 1-15>,
  "phonetic_similarity": {{"Light": <0.0-1.0>, "Medium": <0.0-1.0>, "Far": <0.0-1.0>}},
  "orthographic_similarity": {{"Light": <0.0-1.0>, "Medium": <0.0-1.0>, "Far": <0.0-1.0>}},
  "rule_based_rules": [<up to 3 identifiers from the list below>],
  "rule_percentage": <0.0 if not mentioned in query, otherwise 0.1-0.6>
}}

Rules:
- Use ONLY explicitly stated values. If values are given as percents, convert to decimals 0.0-1.0.
- If orthographic similarity is mentioned without per-level numbers, set {{"Light": 1.0}} unless the text clearly says significant/major/heavy (then {{"Far": 1.0}}).
- List at most 3 rule identifiers, only if explicitly requested.
- If rule percentage is not mentioned in query, set rule_percentage to 0.0 (don't guess/infer a value)
- For variation_count, extract explicit numbers like "generate 5 variations" or "create 10 names", "10 excutor vectors"
- For rule_based_rules:
  - Only include rules explicitly requested in query
  - Don't infer/guess rules that aren't clearly stated
  - Maximum 3 rules total
  - Must match exactly from the rule ID list
  - If no rules mentioned, return empty list []

- Identifiers must be EXACTLY one of:
{id_list_only}

{examples_text}
Input query:
{query_text}

Output: JSON only, no extra text.
"""


def _extract_json_from_text(text: str) -> Optional[Dict[str, Any]]:
    """Find the last JSON object in the text and parse it."""
    try:
        candidates = re.findall(r"\{[\s\S]*\}", text)
        if not candidates:
            return None
        for candidate in reversed(candidates):
            try:
                return json.loads(candidate)
            except Exception:
                continue
    except Exception:
        return None
    return None


def _normalize_llm_result(obj: Dict[str, Any]) -> Dict[str, Any]:
    """Normalize the LLM JSON into the expected dictionary structure."""
    result: Dict[str, Any] = {
        "variation_count": None,
        "phonetic_similarity": {},
        "orthographic_similarity": {},
        "rule_percentage": None,
        "selected_rules": [],
    }

    # Variation count
    vc = obj.get("variation_count")
    if not vc:
        vc = 5
    elif vc > 15:
        vc = 10
    result["variation_count"] = int(vc)

    def normalize_distribution(dist: Any) -> Dict[str, float]:
        if not isinstance(dist, dict):
            return {}
        values = {k.capitalize(): float(v) for k, v in dist.items() if k and isinstance(v, (int, float))}
        # Keep only recognized levels
        values = {k: v for k, v in values.items() if k in {"Light", "Medium", "Far"}}
        # If any value > 1, assume percentages and normalize by total to 0..1
        if any(v > 1.0 for v in values.values()):
            total = sum(values.values())
            if total > 0:
                values = {k: round(v / total, 6) for k, v in values.items()}
        return values

    result["phonetic_similarity"] = normalize_distribution(obj.get("phonetic_similarity", {}))
    result["orthographic_similarity"] = normalize_distribution(obj.get("orthographic_similarity", {}))

    # Rules: expect identifiers only; drop anything not in allowed list
    selected_rules: List[str] = []
    if isinstance(obj.get("rule_based_rules"), list):
        for item in obj["rule_based_rules"]:
            if isinstance(item, str) and item in RULE_DESCRIPTIONS:
                selected_rules.append(item)
    result["selected_rules"] = selected_rules

    rp = obj.get("rule_percentage")
    if isinstance(rp, (int, float)):
        # If >1, assume percentage and convert to decimal
        if rp == 0.0:
            rp = (len(selected_rules) + 0.1) / result["variation_count"]
        
        result["rule_percentage"] = min(0.6, float(rp) / 100.0 if rp > 1 else float(rp))
    if (result["phonetic_similarity"].get("Light", 0.0) +
        result["phonetic_similarity"].get("Medium", 0.0) +
        result["phonetic_similarity"].get("Far", 0.0)) < 1.0:
        if result["phonetic_similarity"].get("Light", 0.0) == 0.2:
            result["phonetic_similarity"]["Medium"] = 0.6
            result["phonetic_similarity"]["Far"] = 0.2
    return result


async def parse_query_with_llm(
    query_text: str,
    *,
    clients: Optional[List[genai.GenerativeModel]] = None,
    max_retries: int = 1,
) -> Dict[str, Any]:
    """Parse using Gemini LLM into a structured dict. Falls back to empty result on failure."""
    if not clients:
        clients = _initialize_clients()
    
    if not clients:
        # No clients available, return empty result
        return {
            "variation_count": 0,
            "phonetic_similarity": {"Light": 0.0, "Medium": 0.0, "Far": 0.0},
            "orthographic_similarity": {"Light": 0.0, "Medium": 0.0, "Far": 0.0},
            "rule_percentage": 0.0,
        }
    
    prompt = _build_llm_prompt(query_text)
    
    for _ in range(max_retries + 1):
        response = None
        try:
            # Randomly select a client and model
            client = random.choice(clients)
            selected_model = random.choice(AVAILABLE_MODELS)
            
            # Create a new model instance with the selected model
            model = genai.GenerativeModel(selected_model)
            
            # Get response from LLM
            response = await model.generate_content_async(prompt)
            
            # Extract and parse response
            content = response.text
            if not content:
                raise ValueError("Empty response from LLM")
            
            obj = _extract_json_from_text(content)
            
            if obj:
                llm_parsed = _normalize_llm_result(obj)
                return llm_parsed
            
        except Exception as e:
            bt.logging.error(f"Error parsing query with Gemini LLM: {e}")
            if response:
                bt.logging.error(f"Response: {response}")
            continue
    
    # If here, LLM failed - return empty result
    return {
        "variation_count": 0,
        "phonetic_similarity": {"Light": 0.0, "Medium": 0.0, "Far": 0.0},
        "orthographic_similarity": {"Light": 0.0, "Medium": 0.0, "Far": 0.0},
        "rule_percentage": 0.0,
    }


# -----------------------------
# REGEX-BASED PARSING (Fallback)
# -----------------------------

## Regex-based parsing removed per instruction: LLM-only parsing now

async def query_parser(
    query_text: str,
    *,
    clients: Optional[List[genai.GenerativeModel]] = None,
    max_retries: int = 10,
) -> Dict[str, Any]:
    """Public API: LLM-only parsing interface."""
    result = await parse_query_with_llm(
        query_text,
        clients=clients,
        max_retries=max_retries,
    )
    return result

def query_parser_sync(
    query_text: str,
    *,
    clients: Optional[List[genai.GenerativeModel]] = None,
    max_retries: int = 10,
) -> Dict[str, Any]:
    """Synchronous wrapper for parse_query for backwards compatibility."""
    return asyncio.run(query_parser(
        query_text,
        clients=clients,
        max_retries=max_retries,
    ))


__all__ = [
    "query_parser",
    "query_parser_sync", 
]
